# **Product Requirements Document: The Tech Tree**

## **1\. Overview**

**Product Name**: **The Tech Tree**

**Description**: The Tech Tree is an adaptive learning platform designed for quick, bite-sized educational interactions, enabling professional users to rapidly acquire valuable skills. Users bookmark knowledge areas, revisit topics, and explore new technology fields in small, engaging units, powered by an AI-driven conversational learning experience.

### **1.1 Motivation and Background**

In a rapidly changing technological landscape, professionals need continuous learning but often lack time and patience for lengthy traditional courses. Current learning platforms often emphasize rote memorization, leading to superficial understanding. The Tech Tree prioritizes active learning, engagement, and retention through interactive, stateful lessons.

### **1.2 Existing Solutions**

* Khan Academy, Coursera, Udacity, Master's programs: structured but demanding significant time.
* The Tech Tree differentiates by providing bite-sized, adaptive, interactive lessons orchestrated by AI, focusing on real learning effectiveness and retention through active participation and conversation.

### **1.3 Scope**

* Initial focus: STEM, specifically AI technologies
* Future scope: Expand to broader professional and personal topics

### **1.4 Path to Production & AI Orchestration**

The current implementation utilizes cloud-hosted Large Language Models (LLMs) orchestrated via **LangGraph**, a Python library for building stateful, multi-actor applications with LLMs. Specific LangGraph graphs manage the user onboarding, syllabus generation, and lesson interaction flows.

* **Note on Portability:** LangGraph is currently specific to Python and TypeScript. If the application is rebuilt in other languages (e.g., Go, Elixir), alternative graph-based state management libraries or custom solutions suitable for those ecosystems will need to be identified or developed. Future phases might still involve exploring locally hosted or fine-tuned models, integrated within the chosen orchestration framework.

## **2\. Goals and Objectives**

### **2.1 Primary Product Goals**

* **Low barrier, high mastery**: Bite-sized but deep, engaging learning experiences
* **Learning Effectiveness**: Deep, confident understanding through interaction
* **User Growth**: Organic user adoption driven by effectiveness and engagement
* **Long-Term Monetization**: Clearly defined enterprise licensing and individual accounts

### **2.2 User Learning Goals**

* Learners progress confidently up the tech tree
* Continuous assessment through AI-generated quizzes and active exercises within lessons

### **2.3 Experience and Engagement Objectives**

* Quick, chat-based tutorials, exercises, and quizzes integrated into lessons
* Gamification: progress indicators, badges, leaderboards

### **2.4 Adaptivity and Personalization Goals**

* Dynamic difficulty adjustment managed by the Lesson LangGraph based on real-time performance and answer evaluation.
* Custom learning paths generated by the Syllabus LangGraph based on user input and initial assessment (via Onboarding LangGraph).

### **2.5 Business Objectives**

* Enterprise partnerships with dedicated onboarding and progress tracking
* Initial technical viability within 3 months, first paying users within 6 months

### **2.6 Constraints**

* Low-cost LLM models initially
* Zero-budget for first 3 months
* Small development team (1 backend, 1 frontend initially, potentially evolving)

---

## **3\. Target Audience & User Personas**

### **Primary Persona: The Upgrading Professional**

* **Age**: 25â€“45
* **Background**: Early- to mid-career knowledge workers
* **Goals**: Upskill quickly for work, anticipate career changes
* **Challenges**: Short attention spans, limited time, intimidated by formal education
* **Preferences**: Bite-sized, active, conversational, quiz-based, gamified
* **Technical Proficiency**: Moderate to high

---

## **4\. User Stories / Use Cases**

### **4.1 Core Learning Flow**

* User discovers via social media/workplace recommendation
* Registers/Logs in via the Flask frontend.
* Starts with an onboarding assessment for a chosen topic, managed by the Onboarding LangGraph.
* Receives a personalized syllabus (Syllabus -> Module -> Lesson structure) generated or retrieved by the Syllabus LangGraph.
* Engages in interactive lessons driven by the Lesson LangGraph.

### **4.2 Syllabus**

* A syllabus (structured as Modules containing Lessons) is needed for each topic.
* Syllabi can be predefined or dynamically generated by the Syllabus LangGraph based on user topic input and assessment results.
* Syllabi are stored in the SQLite database.
* The structure aligns with the `SYLLABI`, `MODULES`, `LESSONS` tables in the database schema.
* An example syllabus follows:

```(json)
{
  "topic": "Introduction to Quantum Computing",
  "level": "Beginner",
  "duration": "4 weeks",
  "learning_objectives": [
    "Understand the basic principles of quantum mechanics, including superposition and entanglement.",
    "Learn about qubits and their representation on the Bloch sphere.",
    "Become familiar with basic quantum gates and circuits.",
    "Understand the concept of measurement in quantum mechanics.",
    "Explore some basic applications of quantum computing."
  ],
  "modules": [
    {
      "title": "Introduction to Quantum Mechanics",
      "lessons": [
        { "title": "What *is* Superposition? (The Intuition)" },
        { "title": "Representing Superposition: Probability Amplitudes" },
        { "title": "The Qubit: A Superposition Superhero" },
        { "title": "Visualizing Superposition: The Bloch Sphere (Introduction)" },
        { "title": "Superposition vs. Classical Mixtures: A Key Difference" },
        { "title": "Mathematical Notation: Describing Superposition with Ket Notation" },
        { "title": "Creating Superposition: The Hadamard Gate" },
        { "title": "Measuring a Superposition: Collapse of the Wavefunction" },
        { "title": "Why Don't We See Superposition in Everyday Life?" },
        { "title": "Applications of Superposition in Quantum Computing" }
      ]
    },
    {
      "title": "Qubits and Quantum Gates",
      "lessons": [
        { "title": "The Bloch Sphere: A Deeper Dive" },
        { "title": "Single Qubit Gates: Pauli X, Y, Z" },
        { "title": "Rotation Gates: Rx, Ry, Rz" },
        { "title": "The Hadamard Gate: Revisited" },
        { "title": "Phase Shift Gates" }
      ]
    },
    {
        "title": "Entanglement and Multi-Qubit Systems",
        "lessons": [
            { "title": "What is Quantum Entanglement?" },
            { "title": "Representing Entangled States" },
            { "title": "The CNOT Gate: Creating Entanglement" },
            { "title": "Other Two-Qubit Gates (SWAP, CZ)" },
            { "title": "Bell States: Maximally Entangled States" }
        ]
    },
      {
        "title": "Basic Quantum Algorithms and Applications",
        "lessons": [
          { "title": "Introduction to Quantum Algorithms" },
          { "title": "Deutsch's Algorithm: A Simple Example" },
          { "title": "Grover's Search Algorithm (Overview)" },
          { "title": "Quantum Teleportation (Conceptual)" },
          { "title": "Quantum Computing Today and Tomorrow" }

        ]
      }
  ]
}
```

### **4.3 Lessons**

* Each lesson is an interactive, conversational experience managed by the Lesson LangGraph.
* The flow typically involves:
  * Initial exposition or prompt from the AI.
  * User interaction (asking questions, requesting exercises, submitting answers).
  * Intent classification by the LangGraph to determine the next step.
  * Dynamic generation of chat responses, exercises, or assessments by specific LangGraph nodes.
  * Evaluation of user answers with adaptive feedback.
* Lesson state (`lesson_state_json`) and conversation history (`conversation_history`) are stored in the `user_progress` and `conversation_history` tables (SQLite) respectively, allowing users to resume lessons and enabling the AI to maintain context.
* The syllabus might be dynamically modified based on performance during lessons (future goal, requires LangGraph logic).

### **4.4 End-of-lesson Quiz**

* **Trigger**: At any point during a lesson, the user can request an end-of-lesson quiz. This switches the user from "Learning Mode" to "Quiz Mode". A message indicating this mode change should appear in the chat interface.
* **Generation**: The quiz is dynamically generated by a dedicated Quiz LangGraph, ensuring it can differ each time. It considers the lesson's difficulty level and covers the lesson content.
* **Length**: The number of questions depends on the difficulty level:
  * Easiest: 2-3 questions
  * Hardest: Up to 5 questions
* **Interaction**:
  * An LLM evaluates answers. It prompts for more detail if an answer is incomplete but potentially correct. Incorrect answers are marked as wrong, and the quiz proceeds.
  * No hints, help, or explanations are provided during the quiz.
  * The quiz interaction occurs within the main chat interface but represents a distinct state.
* **State & Persistence**:
  * The quiz state is transient and managed separately from the main lesson state.
  * If the user abandons the quiz (e.g., requests to go back to learning mode), the Quiz Mode ends, and the quiz-related chat history for that attempt is cleared from the interface. Starting a new quiz also clears any previous quiz attempt's chat.
  * Quiz questions, answers, and scores are *not* persisted beyond the active quiz session, except for the final outcome.
* **Success & Failure**:
  * Success requires answering all questions correctly.
  * Upon successful completion, a "Lesson Complete" flag is set in the `user_progress` record for that lesson.
  * If unsuccessful, the user is offered two choices:
    * **Retry Failed Areas**: The LLM should tag questions by sub-topic. A retry quiz focuses specifically on questions related to the sub-topics of the incorrectly answered questions.
    * **Return to Learning Mode**: Exits Quiz Mode.
* **Technical Implementation**: Managed by a new, separate "Quiz LangGraph" to keep complexity separate from the main "Lesson LangGraph". Quiz performance does not currently feed back into the adaptive difficulty calculations for the lesson itself.

### **4.5 Bookmarking and Returning**

* Lesson progress and state are automatically saved in the database (`user_progress` table), allowing users to resume.
* Manual bookmarking/favorites could be added as a feature.

### **4.6 Adaptive Difficulty**

* The Lesson LangGraph manages adaptive difficulty by:
  * Evaluating user responses to exercises/assessments.
  * Adjusting subsequent questions or explanations based on the stored lesson state and performance.
  * Potentially prompting users to revisit foundational topics if needed.

### **4.7 Progress Tracking / Gamification**

* Visual progress bars (calculated from `user_progress` data), badges, milestones (Bronze, Silver, Gold mastery).
* Leaderboards for motivation.

### **4.8 Enterprise Use Cases**

* Manager-defined syllabus, uploaded internal materials (tools, market data) - *Requires implementation*.
* Enterprise dashboards for progress tracking - *Requires implementation*.

### **4.9 Defining New Topics**

* Users initiate learning on a new topic.
* The Onboarding LangGraph assesses initial knowledge.
* The Syllabus LangGraph generates a personalized syllabus based on the topic and assessment.

---

## **5\. Key Features & Functionality**

* **LangGraph-driven Adaptive Learning Engine**: Manages onboarding, syllabus generation, and lesson interactions using stateful AI graphs.
* **Syllabus Management**: Predefined or dynamically generated learning paths (Syllabus -> Module -> Lesson).
* **Stateful Lesson Interactions**: Conversational lessons with context maintained via stored state and history.
* **On-Demand Exercises & Assessments**: AI generates exercises/quizzes within the lesson flow based on user requests or progress.
* **AI-Powered Feedback**: Evaluation of user responses with adaptive feedback.
* **Proprietary Content Integration**: Enterprise upload of internal materials (*Requires implementation*).
* **Bookmarking & Resuming**: Automatic progress and state saving via the database.
* **Progress & Gamification**: Badges, progress bars, leaderboards.
* **Enterprise Admin Portal**: Syllabus assignment, analytics, user management (*Requires implementation*).
* **Short Video Content**: Uploaded, or AI-generated (*Future goal*).

---

## **6\. Non-Functional Requirements**

### **6.1 Performance**

* Initial response time under 3 seconds for UI elements; provide streaming updates or loading indicators for slower LLM queries via LangGraph.
* Concurrency: Initial 10 users, scaling target TBD based on SQLite performance under load.

### **6.2 Reliability**

* 99.9% uptime target for frontend/backend services.
* Graceful fallback mechanisms within LangGraph nodes for LLM API outages or errors.

### **6.3 Security and Privacy**

* Encryption at rest (SQLite TDE if enabled/needed) and transit (HTTPS). GDPR/CCPA compliant.
* Minimize personal data transfer to LLM APIs; use anonymized IDs where possible.
* Secure authentication (e.g., password hashing, JWTs).

### **6.4 Scalability**

* **Database:** Uses SQLite. While suitable for initial phases and single-server deployments, high concurrency can become a bottleneck. WAL (Write-Ahead Logging) mode is enabled for better read/write concurrency. Scaling may eventually require migration to a client-server database (e.g., PostgreSQL).
* **Backend (FastAPI):** Can be scaled horizontally behind a load balancer if needed.
* **Frontend (Flask):** Can be scaled horizontally using a production server (e.g., Gunicorn/uWSGI) and load balancer.
* **LangGraph/LLMs:** Scaling depends on the underlying LLM API limits and the efficiency of graph execution.

### **6.5 Maintainability**

* Modular backend (FastAPI routers, distinct LangGraph services).
* API-first approach between Flask frontend and FastAPI backend.
* Automated CI/CD pipeline.
* Code linting and formatting enforced (e.g., via pre-commit hooks).

### **6.6 Accessibility**

* WCAG 2.1 AA compliance target for the Flask frontend templates.

### **6.7 Analytics**

* Usage and performance monitoring (e.g., Prometheus/Grafana, Datadog). Log aggregation.

---

## **7\. Technical Approach & Architecture**

The application follows a client-server architecture with a Python-based stack, utilizing Django for the frontend and ORM, and `django-background-tasks` for asynchronous processing and LLM interactions.

### **Conceptual Flow**

```mermaid
graph LR
    A[User (Browser)] -- HTTP --> B(Django Frontend);
    B -- Django Views --> C(Django ORM);
    C -- Reads/Writes --> D[(SQLite DB)];
    B -- Triggers --> E(django-background-tasks);
    E -- Processes Tasks --> F(Background Worker);
    F -- Uses --> G(LangGraph Services);
    G -- Orchestrates --> H(Onboarding Graph);
    G -- Orchestrates --> I(Syllabus Graph);
    G -- Orchestrates --> J(Lesson Graph);
    G -- Orchestrates --> K(Quiz Graph); %% Added Quiz Graph
    H -- Calls --> L{LLM API};
    I -- Calls --> L;
    J -- Calls --> L;
    K -- Calls --> L; %% Added Quiz Graph LLM Call
    F -- Reads/Writes --> D; %% Background worker interacts with DB
```

### **Frontend**

* **Framework**: **Django (Python)**
* **Functionality**: Serves HTML templates (using Jinja2), handles user sessions, manages URL routing, processes form submissions, and uses HTMX for dynamic content updates and asynchronous interactions with Django views.
* **Styling/JS**: Standard CSS and JavaScript for interactivity.

### **Backend / Asynchronous Processing**

* **Framework**: **Django (Python)** for core logic and ORM.
* **Asynchronous Tasks**: **`django-background-tasks`** is used to offload long-running or blocking operations, particularly interactions with LLMs and LangGraph.
* **Task Processors**: Synchronous Python functions (`taskqueue/processors/`) that execute within the background worker process, handling specific task types (e.g., lesson interaction, syllabus generation, quiz processing).

### **AI & LLM Services**

* **Orchestration**: **LangGraph (Python)** library is used within the background task processors to build and run stateful AI agents/graphs for:
  * User Onboarding & Assessment
  * Syllabus Generation & Retrieval
  * Interactive Lesson Flow (chat, exercises, evaluation)
  * End-of-Lesson Quiz (New Quiz Graph)
* **LLMs**: Graphs interact with cloud-hosted LLMs (e.g., GPT models, Gemini) via API calls from within LangGraph nodes, executed by the background worker.
* **Portability Note**: LangGraph is Python/TypeScript specific. Rebuilding in Go/Elixir would require alternative graph/state-management solutions.

### **Data Storage**

* **Database**: **SQLite**
* **Usage**: Stores user accounts, assessments, syllabi (modules, lessons), lesson content, user progress (including serialized lesson state), conversation history, and background task details.
* **Access**: Primarily managed via the Django ORM.

### **Enterprise Admin Tools**

* *To be implemented*, likely as additional Django views, templates, and potentially integrated with the Django Admin site for syllabus creation, content upload, user management, and analytics viewing.

### **Analytics and Logging**

* **Logging**: Centralized logging configured within the Django project.
* **Monitoring**: Real-time monitoring and dashboards (e.g., Prometheus/Grafana, Datadog) integrated with the Django application and background task queue.

---

## **8\. Release Plan & Milestones**

*(Review if milestones align with the implemented Flask/FastAPI/LangGraph/SQLite architecture)*

| Milestone | Description                                     | Date (approx.) |
| :-------- | :---------------------------------------------- | :------------- |
| MVP       | Basic adaptive tutor (Flask/FastAPI/LangGraph) | 3 Months       |
| Beta      | Scaled usage, basic enterprise dashboards       | 6 Months       |
| v1.0      | Monetized enterprise deployments                | 12 Months      |

---

## **9\. Risks & Mitigations**

* **Technical Risks**:
  * **LLM Reliability/Cost**: Dependency on external LLM APIs. *Mitigation*: Caching responses, graceful fallbacks in LangGraph, monitoring costs, exploring alternative models.
  * **SQLite Scalability**: Potential bottleneck under high concurrent write load. *Mitigation*: Enable WAL mode (done), monitor performance, optimize queries, plan for potential migration to PostgreSQL/MySQL if needed.
  * **LangGraph Lock-in**: Python/TS specific library hinders direct porting to Go/Elixir. *Mitigation*: Acknowledge during rebuild planning; identify/develop alternative graph libraries or state management patterns for target languages.
* **Market Risks**:
  * **Initial Adoption**: Gaining traction against established platforms. *Mitigation*: Focus on unique interactive/adaptive features, offer a compelling free tier, build a community.

---

## **10\. KPIs & Success Metrics**

* **User Retention & Engagement**: Daily/Monthly Active Users (DAU/MAU), session duration, lesson completion rates.
* **Learning Progress**: Mastery rates (scores on assessments/exercises), syllabus completions, user feedback on learning effectiveness.
* **Enterprise Adoption**: Number of paying enterprise accounts, revenue generated (target within 6-12 months).
